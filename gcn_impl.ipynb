{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4c6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DGLBACKEND=tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 01:23:49.634261: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/manisb/opt/anaconda3/envs/ml-project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%env DGLBACKEND=tensorflow\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802f6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CiteseerGraphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65409f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dataset[0]\n",
    "adj = graph.adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f19fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = graph.ndata['feat']\n",
    "labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "val_mask = graph.ndata['val_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2887dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GCN_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, adjacency_matrix, input_dim, output_dim):\n",
    "        super(GCN_Layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        a_i = tf.add(\n",
    "            tf.sparse.to_dense(tf.sparse.reorder(adjacency_matrix)), \n",
    "            tf.eye(adjacency_matrix.shape[0]))\n",
    "        self.a_cap = tf.linalg.normalize(a_i, axis=0)[0]\n",
    "        self.w = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.linalg.matmul(tf.linalg.matmul(self.a_cap, inputs), self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071cf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Model(tf.keras.Model):\n",
    "    def __init__(self, adjacency_matrix, imput_dim, output_dim):\n",
    "        super(GCN_Model, self).__init__()\n",
    "        self.layer1 = GCN_Layer(adjacency_matrix, imput_dim, 8*16)\n",
    "        self.layer2 = GCN_Layer(adjacency_matrix, 8*16, output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = tf.keras.activations.relu(self.layer1(inputs))\n",
    "        x2 = self.layer2(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0869b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 1.790, Accuracy: 23.800%\n",
      "Epoch 001: Loss: 1.786, Accuracy: 30.800%\n",
      "Epoch 002: Loss: 1.782, Accuracy: 35.400%\n",
      "Epoch 003: Loss: 1.778, Accuracy: 38.200%\n",
      "Epoch 004: Loss: 1.773, Accuracy: 40.800%\n",
      "Epoch 005: Loss: 1.768, Accuracy: 42.000%\n",
      "Epoch 006: Loss: 1.763, Accuracy: 43.200%\n",
      "Epoch 007: Loss: 1.757, Accuracy: 43.400%\n",
      "Epoch 008: Loss: 1.752, Accuracy: 44.200%\n",
      "Epoch 009: Loss: 1.745, Accuracy: 44.600%\n",
      "Epoch 010: Loss: 1.739, Accuracy: 45.000%\n",
      "Epoch 011: Loss: 1.732, Accuracy: 45.200%\n",
      "Epoch 012: Loss: 1.724, Accuracy: 45.600%\n",
      "Epoch 013: Loss: 1.716, Accuracy: 45.800%\n",
      "Epoch 014: Loss: 1.708, Accuracy: 45.600%\n",
      "Epoch 015: Loss: 1.700, Accuracy: 46.000%\n",
      "Epoch 016: Loss: 1.691, Accuracy: 46.200%\n",
      "Epoch 017: Loss: 1.681, Accuracy: 47.000%\n",
      "Epoch 018: Loss: 1.671, Accuracy: 47.600%\n",
      "Epoch 019: Loss: 1.661, Accuracy: 48.200%\n",
      "Epoch 020: Loss: 1.651, Accuracy: 48.600%\n",
      "Epoch 021: Loss: 1.640, Accuracy: 49.000%\n",
      "Epoch 022: Loss: 1.629, Accuracy: 50.000%\n",
      "Epoch 023: Loss: 1.617, Accuracy: 50.400%\n",
      "Epoch 024: Loss: 1.605, Accuracy: 50.200%\n",
      "Epoch 025: Loss: 1.593, Accuracy: 50.200%\n",
      "Epoch 026: Loss: 1.581, Accuracy: 51.000%\n",
      "Epoch 027: Loss: 1.568, Accuracy: 52.000%\n",
      "Epoch 028: Loss: 1.555, Accuracy: 52.400%\n",
      "Epoch 029: Loss: 1.541, Accuracy: 53.400%\n",
      "Epoch 030: Loss: 1.527, Accuracy: 53.800%\n",
      "Epoch 031: Loss: 1.513, Accuracy: 54.000%\n",
      "Epoch 032: Loss: 1.499, Accuracy: 55.200%\n",
      "Epoch 033: Loss: 1.485, Accuracy: 55.600%\n",
      "Epoch 034: Loss: 1.470, Accuracy: 56.000%\n",
      "Epoch 035: Loss: 1.455, Accuracy: 55.800%\n",
      "Epoch 036: Loss: 1.440, Accuracy: 56.000%\n",
      "Epoch 037: Loss: 1.424, Accuracy: 56.800%\n",
      "Epoch 038: Loss: 1.409, Accuracy: 57.400%\n",
      "Epoch 039: Loss: 1.393, Accuracy: 58.200%\n",
      "Epoch 040: Loss: 1.377, Accuracy: 59.400%\n",
      "Epoch 041: Loss: 1.361, Accuracy: 59.400%\n",
      "Epoch 042: Loss: 1.344, Accuracy: 59.400%\n",
      "Epoch 043: Loss: 1.328, Accuracy: 59.400%\n",
      "Epoch 044: Loss: 1.311, Accuracy: 59.400%\n",
      "Epoch 045: Loss: 1.294, Accuracy: 59.400%\n",
      "Epoch 046: Loss: 1.277, Accuracy: 60.200%\n",
      "Epoch 047: Loss: 1.260, Accuracy: 60.200%\n",
      "Epoch 048: Loss: 1.243, Accuracy: 60.800%\n",
      "Epoch 049: Loss: 1.226, Accuracy: 60.800%\n",
      "Epoch 050: Loss: 1.209, Accuracy: 61.000%\n",
      "Epoch 051: Loss: 1.191, Accuracy: 62.000%\n",
      "Epoch 052: Loss: 1.174, Accuracy: 62.400%\n",
      "Epoch 053: Loss: 1.156, Accuracy: 62.600%\n",
      "Epoch 054: Loss: 1.139, Accuracy: 62.800%\n",
      "Epoch 055: Loss: 1.121, Accuracy: 62.800%\n",
      "Epoch 056: Loss: 1.104, Accuracy: 63.200%\n",
      "Epoch 057: Loss: 1.086, Accuracy: 63.000%\n",
      "Epoch 058: Loss: 1.068, Accuracy: 63.000%\n",
      "Epoch 059: Loss: 1.051, Accuracy: 63.400%\n",
      "Epoch 060: Loss: 1.033, Accuracy: 63.800%\n",
      "Epoch 061: Loss: 1.016, Accuracy: 64.000%\n",
      "Epoch 062: Loss: 0.999, Accuracy: 64.400%\n",
      "Epoch 063: Loss: 0.981, Accuracy: 64.200%\n",
      "Epoch 064: Loss: 0.964, Accuracy: 64.200%\n",
      "Epoch 065: Loss: 0.947, Accuracy: 65.000%\n",
      "Epoch 066: Loss: 0.930, Accuracy: 65.000%\n",
      "Epoch 067: Loss: 0.913, Accuracy: 65.400%\n",
      "Epoch 068: Loss: 0.896, Accuracy: 65.200%\n",
      "Epoch 069: Loss: 0.879, Accuracy: 66.000%\n",
      "Epoch 070: Loss: 0.862, Accuracy: 65.600%\n",
      "Epoch 071: Loss: 0.846, Accuracy: 66.000%\n",
      "Epoch 072: Loss: 0.830, Accuracy: 66.200%\n",
      "Epoch 073: Loss: 0.814, Accuracy: 66.200%\n",
      "Epoch 074: Loss: 0.798, Accuracy: 66.400%\n",
      "Epoch 075: Loss: 0.782, Accuracy: 66.600%\n",
      "Epoch 076: Loss: 0.766, Accuracy: 66.400%\n",
      "Epoch 077: Loss: 0.751, Accuracy: 66.600%\n",
      "Epoch 078: Loss: 0.735, Accuracy: 67.000%\n",
      "Epoch 079: Loss: 0.720, Accuracy: 66.600%\n",
      "Epoch 080: Loss: 0.705, Accuracy: 66.600%\n",
      "Epoch 081: Loss: 0.691, Accuracy: 66.800%\n",
      "Epoch 082: Loss: 0.676, Accuracy: 66.400%\n",
      "Epoch 083: Loss: 0.662, Accuracy: 66.400%\n",
      "Epoch 084: Loss: 0.648, Accuracy: 66.600%\n",
      "Epoch 085: Loss: 0.634, Accuracy: 66.600%\n",
      "Epoch 086: Loss: 0.620, Accuracy: 66.800%\n",
      "Epoch 087: Loss: 0.607, Accuracy: 66.800%\n",
      "Epoch 088: Loss: 0.593, Accuracy: 66.600%\n",
      "Epoch 089: Loss: 0.580, Accuracy: 66.600%\n",
      "Epoch 090: Loss: 0.567, Accuracy: 66.600%\n",
      "Epoch 091: Loss: 0.555, Accuracy: 66.600%\n",
      "Epoch 092: Loss: 0.542, Accuracy: 66.600%\n",
      "Epoch 093: Loss: 0.530, Accuracy: 66.600%\n",
      "Epoch 094: Loss: 0.518, Accuracy: 66.600%\n",
      "Epoch 095: Loss: 0.507, Accuracy: 66.600%\n",
      "Epoch 096: Loss: 0.495, Accuracy: 66.600%\n",
      "Epoch 097: Loss: 0.484, Accuracy: 66.600%\n",
      "Epoch 098: Loss: 0.473, Accuracy: 66.600%\n",
      "Epoch 099: Loss: 0.462, Accuracy: 66.600%\n",
      "Epoch 100: Loss: 0.452, Accuracy: 66.400%\n",
      "Epoch 101: Loss: 0.441, Accuracy: 66.600%\n",
      "Epoch 102: Loss: 0.431, Accuracy: 66.400%\n",
      "Epoch 103: Loss: 0.421, Accuracy: 66.200%\n",
      "Epoch 104: Loss: 0.411, Accuracy: 66.200%\n",
      "Epoch 105: Loss: 0.402, Accuracy: 66.200%\n",
      "Epoch 106: Loss: 0.393, Accuracy: 66.000%\n",
      "Epoch 107: Loss: 0.383, Accuracy: 66.000%\n",
      "Epoch 108: Loss: 0.375, Accuracy: 66.000%\n",
      "Epoch 109: Loss: 0.366, Accuracy: 66.000%\n",
      "Epoch 110: Loss: 0.357, Accuracy: 66.000%\n",
      "Epoch 111: Loss: 0.349, Accuracy: 66.000%\n",
      "Epoch 112: Loss: 0.341, Accuracy: 66.200%\n",
      "Epoch 113: Loss: 0.333, Accuracy: 66.200%\n",
      "Epoch 114: Loss: 0.325, Accuracy: 66.200%\n",
      "Epoch 115: Loss: 0.318, Accuracy: 66.200%\n",
      "Epoch 116: Loss: 0.310, Accuracy: 66.200%\n",
      "Epoch 117: Loss: 0.303, Accuracy: 66.000%\n",
      "Epoch 118: Loss: 0.296, Accuracy: 65.800%\n",
      "Epoch 119: Loss: 0.290, Accuracy: 65.800%\n",
      "Epoch 120: Loss: 0.283, Accuracy: 65.800%\n",
      "Epoch 121: Loss: 0.276, Accuracy: 65.800%\n",
      "Epoch 122: Loss: 0.270, Accuracy: 65.800%\n",
      "Epoch 123: Loss: 0.264, Accuracy: 65.800%\n",
      "Epoch 124: Loss: 0.258, Accuracy: 66.200%\n",
      "Epoch 125: Loss: 0.252, Accuracy: 66.000%\n",
      "Epoch 126: Loss: 0.246, Accuracy: 65.800%\n",
      "Epoch 127: Loss: 0.241, Accuracy: 65.800%\n",
      "Epoch 128: Loss: 0.236, Accuracy: 65.800%\n",
      "Epoch 129: Loss: 0.230, Accuracy: 65.800%\n",
      "Epoch 130: Loss: 0.225, Accuracy: 65.800%\n",
      "Epoch 131: Loss: 0.220, Accuracy: 65.800%\n",
      "Epoch 132: Loss: 0.215, Accuracy: 66.000%\n",
      "Epoch 133: Loss: 0.211, Accuracy: 66.000%\n",
      "Epoch 134: Loss: 0.206, Accuracy: 66.000%\n",
      "Epoch 135: Loss: 0.202, Accuracy: 66.000%\n",
      "Epoch 136: Loss: 0.197, Accuracy: 66.000%\n",
      "Epoch 137: Loss: 0.193, Accuracy: 66.000%\n",
      "Epoch 138: Loss: 0.189, Accuracy: 66.000%\n",
      "Epoch 139: Loss: 0.185, Accuracy: 65.800%\n",
      "Epoch 140: Loss: 0.181, Accuracy: 65.800%\n",
      "Epoch 141: Loss: 0.177, Accuracy: 65.800%\n",
      "Epoch 142: Loss: 0.174, Accuracy: 65.800%\n",
      "Epoch 143: Loss: 0.170, Accuracy: 65.800%\n",
      "Epoch 144: Loss: 0.167, Accuracy: 65.800%\n",
      "Epoch 145: Loss: 0.163, Accuracy: 65.800%\n",
      "Epoch 146: Loss: 0.160, Accuracy: 65.800%\n",
      "Epoch 147: Loss: 0.157, Accuracy: 65.800%\n",
      "Epoch 148: Loss: 0.153, Accuracy: 65.800%\n",
      "Epoch 149: Loss: 0.150, Accuracy: 65.800%\n",
      "Epoch 150: Loss: 0.147, Accuracy: 66.000%\n",
      "Epoch 151: Loss: 0.145, Accuracy: 66.000%\n",
      "Epoch 152: Loss: 0.142, Accuracy: 66.000%\n",
      "Epoch 153: Loss: 0.139, Accuracy: 66.000%\n",
      "Epoch 154: Loss: 0.136, Accuracy: 66.000%\n",
      "Epoch 155: Loss: 0.134, Accuracy: 66.000%\n",
      "Epoch 156: Loss: 0.131, Accuracy: 66.000%\n",
      "Epoch 157: Loss: 0.129, Accuracy: 65.800%\n",
      "Epoch 158: Loss: 0.126, Accuracy: 65.800%\n",
      "Epoch 159: Loss: 0.124, Accuracy: 65.800%\n",
      "Epoch 160: Loss: 0.122, Accuracy: 65.800%\n",
      "Epoch 161: Loss: 0.119, Accuracy: 65.800%\n",
      "Epoch 162: Loss: 0.117, Accuracy: 65.800%\n",
      "Epoch 163: Loss: 0.115, Accuracy: 65.800%\n",
      "Epoch 164: Loss: 0.113, Accuracy: 65.800%\n",
      "Epoch 165: Loss: 0.111, Accuracy: 65.800%\n",
      "Epoch 166: Loss: 0.109, Accuracy: 65.800%\n",
      "Epoch 167: Loss: 0.107, Accuracy: 66.000%\n",
      "Epoch 168: Loss: 0.105, Accuracy: 66.000%\n",
      "Epoch 169: Loss: 0.104, Accuracy: 66.000%\n",
      "Epoch 170: Loss: 0.102, Accuracy: 66.000%\n",
      "Epoch 171: Loss: 0.100, Accuracy: 66.000%\n",
      "Epoch 172: Loss: 0.098, Accuracy: 66.000%\n",
      "Epoch 173: Loss: 0.097, Accuracy: 66.000%\n",
      "Epoch 174: Loss: 0.095, Accuracy: 66.000%\n",
      "Epoch 175: Loss: 0.094, Accuracy: 66.000%\n",
      "Epoch 176: Loss: 0.092, Accuracy: 66.000%\n",
      "Epoch 177: Loss: 0.091, Accuracy: 66.000%\n",
      "Epoch 178: Loss: 0.089, Accuracy: 66.000%\n",
      "Epoch 179: Loss: 0.088, Accuracy: 66.000%\n",
      "Epoch 180: Loss: 0.086, Accuracy: 66.000%\n",
      "Epoch 181: Loss: 0.085, Accuracy: 66.000%\n",
      "Epoch 182: Loss: 0.084, Accuracy: 66.000%\n",
      "Epoch 183: Loss: 0.082, Accuracy: 66.200%\n",
      "Epoch 184: Loss: 0.081, Accuracy: 66.400%\n",
      "Epoch 185: Loss: 0.080, Accuracy: 66.400%\n",
      "Epoch 186: Loss: 0.079, Accuracy: 66.400%\n",
      "Epoch 187: Loss: 0.077, Accuracy: 66.400%\n",
      "Epoch 188: Loss: 0.076, Accuracy: 66.400%\n",
      "Epoch 189: Loss: 0.075, Accuracy: 66.400%\n",
      "Epoch 190: Loss: 0.074, Accuracy: 66.400%\n",
      "Epoch 191: Loss: 0.073, Accuracy: 66.600%\n",
      "Epoch 192: Loss: 0.072, Accuracy: 66.400%\n",
      "Epoch 193: Loss: 0.071, Accuracy: 66.400%\n",
      "Epoch 194: Loss: 0.070, Accuracy: 66.400%\n",
      "Epoch 195: Loss: 0.069, Accuracy: 66.400%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196: Loss: 0.068, Accuracy: 66.400%\n",
      "Epoch 197: Loss: 0.067, Accuracy: 66.400%\n",
      "Epoch 198: Loss: 0.066, Accuracy: 66.400%\n",
      "Epoch 199: Loss: 0.065, Accuracy: 66.400%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "model = GCN_Model(adj, features.shape[1], tf.unique(labels)[0].shape[0])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, x, y, train_mask, training):\n",
    "    y_pre = model(x)\n",
    "    return loss_object(y_true=y[train_mask], y_pred=y_pre[train_mask])\n",
    "\n",
    "def grad(model, inputs, targets, train_mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, train_mask, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "for epoch in range(200):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_value = loss(model, tf.cast(features, dtype=tf.float32), labels, train_mask, training=True)\n",
    "    loss_value, grads = grad(model, tf.cast(features, dtype=tf.float32), labels, train_mask)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    epoch_loss_avg.update_state(loss_value)\n",
    "    epoch_accuracy.update_state(labels[val_mask], model(features, training=True)[val_mask])\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9623403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 65.800%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "prediction = tf.argmax(model(features, training=False)[test_mask], axis=1, output_type=tf.int64)\n",
    "test_accuracy(prediction, labels[test_mask])\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
