{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4c6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DGLBACKEND=tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 00:58:49.305409: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/manisb/opt/anaconda3/envs/ml-project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%env DGLBACKEND=tensorflow\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802f6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CiteseerGraphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65409f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dataset[0]\n",
    "adj = graph.adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f19fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = graph.ndata['feat']\n",
    "labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "val_mask = graph.ndata['val_mask']\n",
    "\n",
    "x = features[train_mask]\n",
    "tx = features[test_mask]\n",
    "allx = features[:]\n",
    "y = labels[train_mask]\n",
    "ty = labels[test_mask]\n",
    "ally = labels[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2887dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GCN_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, adjacency_matrix, input_dim, output_dim):\n",
    "        super(GCN_Layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        a_i = tf.add(\n",
    "            tf.sparse.to_dense(tf.sparse.reorder(adjacency_matrix)), \n",
    "            tf.eye(adjacency_matrix.shape[0]))\n",
    "        self.a_cap = tf.linalg.normalize(a_i, axis=0)[0]\n",
    "        self.w = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.linalg.matmul(tf.linalg.matmul(self.a_cap, inputs), self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071cf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Model(tf.keras.Model):\n",
    "    def __init__(self, adjacency_matrix, imput_dim, output_dim):\n",
    "        super(GCN_Model, self).__init__()\n",
    "        self.layer1 = GCN_Layer(adjacency_matrix, imput_dim, 8*16)\n",
    "        self.layer2 = GCN_Layer(adjacency_matrix, 8*16, output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = tf.keras.activations.relu(self.layer1(inputs))\n",
    "        x2 = self.layer2(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0869b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 1.792, Accuracy: 28.200%\n",
      "Epoch 001: Loss: 1.788, Accuracy: 39.400%\n",
      "Epoch 002: Loss: 1.783, Accuracy: 42.200%\n",
      "Epoch 003: Loss: 1.778, Accuracy: 43.600%\n",
      "Epoch 004: Loss: 1.773, Accuracy: 43.200%\n",
      "Epoch 005: Loss: 1.768, Accuracy: 44.000%\n",
      "Epoch 006: Loss: 1.763, Accuracy: 43.600%\n",
      "Epoch 007: Loss: 1.758, Accuracy: 43.800%\n",
      "Epoch 008: Loss: 1.752, Accuracy: 44.600%\n",
      "Epoch 009: Loss: 1.745, Accuracy: 45.200%\n",
      "Epoch 010: Loss: 1.739, Accuracy: 45.600%\n",
      "Epoch 011: Loss: 1.732, Accuracy: 45.400%\n",
      "Epoch 012: Loss: 1.725, Accuracy: 45.400%\n",
      "Epoch 013: Loss: 1.717, Accuracy: 46.000%\n",
      "Epoch 014: Loss: 1.709, Accuracy: 46.200%\n",
      "Epoch 015: Loss: 1.700, Accuracy: 46.200%\n",
      "Epoch 016: Loss: 1.691, Accuracy: 46.400%\n",
      "Epoch 017: Loss: 1.682, Accuracy: 46.200%\n",
      "Epoch 018: Loss: 1.672, Accuracy: 46.600%\n",
      "Epoch 019: Loss: 1.662, Accuracy: 46.800%\n",
      "Epoch 020: Loss: 1.652, Accuracy: 47.200%\n",
      "Epoch 021: Loss: 1.641, Accuracy: 47.600%\n",
      "Epoch 022: Loss: 1.630, Accuracy: 48.400%\n",
      "Epoch 023: Loss: 1.618, Accuracy: 49.200%\n",
      "Epoch 024: Loss: 1.607, Accuracy: 50.000%\n",
      "Epoch 025: Loss: 1.594, Accuracy: 51.000%\n",
      "Epoch 026: Loss: 1.582, Accuracy: 51.400%\n",
      "Epoch 027: Loss: 1.569, Accuracy: 51.800%\n",
      "Epoch 028: Loss: 1.556, Accuracy: 51.800%\n",
      "Epoch 029: Loss: 1.543, Accuracy: 51.800%\n",
      "Epoch 030: Loss: 1.529, Accuracy: 52.000%\n",
      "Epoch 031: Loss: 1.515, Accuracy: 52.200%\n",
      "Epoch 032: Loss: 1.501, Accuracy: 52.400%\n",
      "Epoch 033: Loss: 1.487, Accuracy: 52.600%\n",
      "Epoch 034: Loss: 1.472, Accuracy: 53.200%\n",
      "Epoch 035: Loss: 1.457, Accuracy: 53.600%\n",
      "Epoch 036: Loss: 1.442, Accuracy: 54.200%\n",
      "Epoch 037: Loss: 1.426, Accuracy: 54.600%\n",
      "Epoch 038: Loss: 1.411, Accuracy: 55.200%\n",
      "Epoch 039: Loss: 1.395, Accuracy: 55.600%\n",
      "Epoch 040: Loss: 1.379, Accuracy: 55.800%\n",
      "Epoch 041: Loss: 1.363, Accuracy: 56.200%\n",
      "Epoch 042: Loss: 1.347, Accuracy: 56.400%\n",
      "Epoch 043: Loss: 1.331, Accuracy: 57.400%\n",
      "Epoch 044: Loss: 1.314, Accuracy: 57.400%\n",
      "Epoch 045: Loss: 1.298, Accuracy: 58.000%\n",
      "Epoch 046: Loss: 1.281, Accuracy: 58.600%\n",
      "Epoch 047: Loss: 1.264, Accuracy: 58.600%\n",
      "Epoch 048: Loss: 1.247, Accuracy: 58.200%\n",
      "Epoch 049: Loss: 1.230, Accuracy: 59.000%\n",
      "Epoch 050: Loss: 1.213, Accuracy: 59.600%\n",
      "Epoch 051: Loss: 1.196, Accuracy: 60.200%\n",
      "Epoch 052: Loss: 1.179, Accuracy: 60.200%\n",
      "Epoch 053: Loss: 1.162, Accuracy: 60.200%\n",
      "Epoch 054: Loss: 1.144, Accuracy: 60.600%\n",
      "Epoch 055: Loss: 1.127, Accuracy: 61.800%\n",
      "Epoch 056: Loss: 1.110, Accuracy: 62.200%\n",
      "Epoch 057: Loss: 1.092, Accuracy: 62.200%\n",
      "Epoch 058: Loss: 1.075, Accuracy: 63.000%\n",
      "Epoch 059: Loss: 1.058, Accuracy: 63.200%\n",
      "Epoch 060: Loss: 1.041, Accuracy: 63.000%\n",
      "Epoch 061: Loss: 1.023, Accuracy: 62.800%\n",
      "Epoch 062: Loss: 1.006, Accuracy: 62.600%\n",
      "Epoch 063: Loss: 0.989, Accuracy: 63.800%\n",
      "Epoch 064: Loss: 0.972, Accuracy: 64.400%\n",
      "Epoch 065: Loss: 0.955, Accuracy: 65.000%\n",
      "Epoch 066: Loss: 0.938, Accuracy: 64.800%\n",
      "Epoch 067: Loss: 0.921, Accuracy: 64.800%\n",
      "Epoch 068: Loss: 0.905, Accuracy: 65.000%\n",
      "Epoch 069: Loss: 0.888, Accuracy: 65.600%\n",
      "Epoch 070: Loss: 0.872, Accuracy: 65.600%\n",
      "Epoch 071: Loss: 0.855, Accuracy: 65.800%\n",
      "Epoch 072: Loss: 0.839, Accuracy: 65.800%\n",
      "Epoch 073: Loss: 0.823, Accuracy: 65.800%\n",
      "Epoch 074: Loss: 0.807, Accuracy: 66.000%\n",
      "Epoch 075: Loss: 0.792, Accuracy: 66.200%\n",
      "Epoch 076: Loss: 0.776, Accuracy: 66.400%\n",
      "Epoch 077: Loss: 0.761, Accuracy: 66.800%\n",
      "Epoch 078: Loss: 0.745, Accuracy: 66.800%\n",
      "Epoch 079: Loss: 0.730, Accuracy: 67.000%\n",
      "Epoch 080: Loss: 0.716, Accuracy: 67.000%\n",
      "Epoch 081: Loss: 0.701, Accuracy: 66.600%\n",
      "Epoch 082: Loss: 0.687, Accuracy: 66.600%\n",
      "Epoch 083: Loss: 0.672, Accuracy: 66.600%\n",
      "Epoch 084: Loss: 0.658, Accuracy: 66.400%\n",
      "Epoch 085: Loss: 0.644, Accuracy: 66.400%\n",
      "Epoch 086: Loss: 0.631, Accuracy: 66.400%\n",
      "Epoch 087: Loss: 0.617, Accuracy: 66.600%\n",
      "Epoch 088: Loss: 0.604, Accuracy: 66.800%\n",
      "Epoch 089: Loss: 0.591, Accuracy: 66.800%\n",
      "Epoch 090: Loss: 0.578, Accuracy: 66.800%\n",
      "Epoch 091: Loss: 0.566, Accuracy: 66.800%\n",
      "Epoch 092: Loss: 0.553, Accuracy: 66.800%\n",
      "Epoch 093: Loss: 0.541, Accuracy: 66.800%\n",
      "Epoch 094: Loss: 0.529, Accuracy: 66.800%\n",
      "Epoch 095: Loss: 0.518, Accuracy: 66.800%\n",
      "Epoch 096: Loss: 0.506, Accuracy: 67.000%\n",
      "Epoch 097: Loss: 0.495, Accuracy: 67.000%\n",
      "Epoch 098: Loss: 0.484, Accuracy: 67.200%\n",
      "Epoch 099: Loss: 0.473, Accuracy: 67.200%\n",
      "Epoch 100: Loss: 0.462, Accuracy: 67.200%\n",
      "Epoch 101: Loss: 0.452, Accuracy: 67.000%\n",
      "Epoch 102: Loss: 0.442, Accuracy: 67.000%\n",
      "Epoch 103: Loss: 0.432, Accuracy: 67.000%\n",
      "Epoch 104: Loss: 0.422, Accuracy: 67.200%\n",
      "Epoch 105: Loss: 0.413, Accuracy: 67.200%\n",
      "Epoch 106: Loss: 0.403, Accuracy: 67.000%\n",
      "Epoch 107: Loss: 0.394, Accuracy: 66.600%\n",
      "Epoch 108: Loss: 0.385, Accuracy: 66.600%\n",
      "Epoch 109: Loss: 0.376, Accuracy: 67.000%\n",
      "Epoch 110: Loss: 0.368, Accuracy: 67.000%\n",
      "Epoch 111: Loss: 0.359, Accuracy: 67.400%\n",
      "Epoch 112: Loss: 0.351, Accuracy: 67.000%\n",
      "Epoch 113: Loss: 0.343, Accuracy: 67.000%\n",
      "Epoch 114: Loss: 0.335, Accuracy: 67.200%\n",
      "Epoch 115: Loss: 0.328, Accuracy: 67.200%\n",
      "Epoch 116: Loss: 0.320, Accuracy: 67.200%\n",
      "Epoch 117: Loss: 0.313, Accuracy: 67.200%\n",
      "Epoch 118: Loss: 0.306, Accuracy: 67.200%\n",
      "Epoch 119: Loss: 0.299, Accuracy: 67.200%\n",
      "Epoch 120: Loss: 0.293, Accuracy: 67.200%\n",
      "Epoch 121: Loss: 0.286, Accuracy: 67.200%\n",
      "Epoch 122: Loss: 0.280, Accuracy: 66.800%\n",
      "Epoch 123: Loss: 0.273, Accuracy: 66.800%\n",
      "Epoch 124: Loss: 0.267, Accuracy: 67.000%\n",
      "Epoch 125: Loss: 0.261, Accuracy: 67.200%\n",
      "Epoch 126: Loss: 0.256, Accuracy: 67.000%\n",
      "Epoch 127: Loss: 0.250, Accuracy: 67.000%\n",
      "Epoch 128: Loss: 0.244, Accuracy: 67.200%\n",
      "Epoch 129: Loss: 0.239, Accuracy: 67.200%\n",
      "Epoch 130: Loss: 0.234, Accuracy: 67.400%\n",
      "Epoch 131: Loss: 0.229, Accuracy: 67.400%\n",
      "Epoch 132: Loss: 0.224, Accuracy: 67.600%\n",
      "Epoch 133: Loss: 0.219, Accuracy: 67.400%\n",
      "Epoch 134: Loss: 0.214, Accuracy: 67.400%\n",
      "Epoch 135: Loss: 0.210, Accuracy: 67.400%\n",
      "Epoch 136: Loss: 0.205, Accuracy: 67.400%\n",
      "Epoch 137: Loss: 0.201, Accuracy: 67.600%\n",
      "Epoch 138: Loss: 0.197, Accuracy: 67.600%\n",
      "Epoch 139: Loss: 0.193, Accuracy: 67.600%\n",
      "Epoch 140: Loss: 0.189, Accuracy: 67.800%\n",
      "Epoch 141: Loss: 0.185, Accuracy: 67.800%\n",
      "Epoch 142: Loss: 0.181, Accuracy: 67.800%\n",
      "Epoch 143: Loss: 0.177, Accuracy: 67.800%\n",
      "Epoch 144: Loss: 0.174, Accuracy: 67.800%\n",
      "Epoch 145: Loss: 0.170, Accuracy: 67.800%\n",
      "Epoch 146: Loss: 0.167, Accuracy: 67.600%\n",
      "Epoch 147: Loss: 0.164, Accuracy: 67.600%\n",
      "Epoch 148: Loss: 0.160, Accuracy: 67.600%\n",
      "Epoch 149: Loss: 0.157, Accuracy: 67.600%\n",
      "Epoch 150: Loss: 0.154, Accuracy: 67.600%\n",
      "Epoch 151: Loss: 0.151, Accuracy: 67.600%\n",
      "Epoch 152: Loss: 0.148, Accuracy: 67.400%\n",
      "Epoch 153: Loss: 0.145, Accuracy: 67.400%\n",
      "Epoch 154: Loss: 0.143, Accuracy: 67.400%\n",
      "Epoch 155: Loss: 0.140, Accuracy: 67.400%\n",
      "Epoch 156: Loss: 0.137, Accuracy: 67.400%\n",
      "Epoch 157: Loss: 0.135, Accuracy: 67.400%\n",
      "Epoch 158: Loss: 0.132, Accuracy: 67.400%\n",
      "Epoch 159: Loss: 0.130, Accuracy: 67.400%\n",
      "Epoch 160: Loss: 0.128, Accuracy: 67.400%\n",
      "Epoch 161: Loss: 0.125, Accuracy: 67.400%\n",
      "Epoch 162: Loss: 0.123, Accuracy: 67.400%\n",
      "Epoch 163: Loss: 0.121, Accuracy: 67.400%\n",
      "Epoch 164: Loss: 0.119, Accuracy: 67.400%\n",
      "Epoch 165: Loss: 0.117, Accuracy: 67.400%\n",
      "Epoch 166: Loss: 0.115, Accuracy: 67.400%\n",
      "Epoch 167: Loss: 0.113, Accuracy: 67.400%\n",
      "Epoch 168: Loss: 0.111, Accuracy: 67.400%\n",
      "Epoch 169: Loss: 0.109, Accuracy: 67.400%\n",
      "Epoch 170: Loss: 0.107, Accuracy: 67.400%\n",
      "Epoch 171: Loss: 0.105, Accuracy: 67.400%\n",
      "Epoch 172: Loss: 0.103, Accuracy: 67.400%\n",
      "Epoch 173: Loss: 0.102, Accuracy: 67.400%\n",
      "Epoch 174: Loss: 0.100, Accuracy: 67.400%\n",
      "Epoch 175: Loss: 0.098, Accuracy: 67.400%\n",
      "Epoch 176: Loss: 0.097, Accuracy: 67.400%\n",
      "Epoch 177: Loss: 0.095, Accuracy: 67.400%\n",
      "Epoch 178: Loss: 0.094, Accuracy: 67.400%\n",
      "Epoch 179: Loss: 0.092, Accuracy: 67.400%\n",
      "Epoch 180: Loss: 0.091, Accuracy: 67.400%\n",
      "Epoch 181: Loss: 0.089, Accuracy: 67.400%\n",
      "Epoch 182: Loss: 0.088, Accuracy: 67.400%\n",
      "Epoch 183: Loss: 0.086, Accuracy: 67.400%\n",
      "Epoch 184: Loss: 0.085, Accuracy: 67.400%\n",
      "Epoch 185: Loss: 0.084, Accuracy: 67.400%\n",
      "Epoch 186: Loss: 0.083, Accuracy: 67.400%\n",
      "Epoch 187: Loss: 0.081, Accuracy: 67.400%\n",
      "Epoch 188: Loss: 0.080, Accuracy: 67.400%\n",
      "Epoch 189: Loss: 0.079, Accuracy: 67.200%\n",
      "Epoch 190: Loss: 0.078, Accuracy: 67.200%\n",
      "Epoch 191: Loss: 0.077, Accuracy: 67.200%\n",
      "Epoch 192: Loss: 0.076, Accuracy: 67.200%\n",
      "Epoch 193: Loss: 0.074, Accuracy: 67.400%\n",
      "Epoch 194: Loss: 0.073, Accuracy: 67.400%\n",
      "Epoch 195: Loss: 0.072, Accuracy: 67.400%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196: Loss: 0.071, Accuracy: 67.400%\n",
      "Epoch 197: Loss: 0.070, Accuracy: 67.400%\n",
      "Epoch 198: Loss: 0.069, Accuracy: 67.200%\n",
      "Epoch 199: Loss: 0.068, Accuracy: 67.200%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "model = GCN_Model(adj, features.shape[1], tf.unique(labels)[0].shape[0])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, x, y, train_mask, training):\n",
    "    y_pre = model(x)\n",
    "    return loss_object(y_true=y[train_mask], y_pred=y_pre[train_mask])\n",
    "\n",
    "def grad(model, inputs, targets, train_mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, train_mask, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "for epoch in range(200):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_value = loss(model, tf.cast(features, dtype=tf.float32), labels, train_mask, training=True)\n",
    "    loss_value, grads = grad(model, tf.cast(features, dtype=tf.float32), labels, train_mask)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    epoch_loss_avg.update_state(loss_value)\n",
    "    epoch_accuracy.update_state(labels[val_mask], model(features, training=True)[val_mask])\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9623403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 65.800%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "prediction = tf.argmax(model(features, training=False)[test_mask], axis=1, output_type=tf.int64)\n",
    "test_accuracy(prediction, labels[test_mask])\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
